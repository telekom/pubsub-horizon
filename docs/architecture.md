# System Architecture
In the picture below you can see the hole system architecture of Horizon.
It should be read from any entrypoint (Starlight, Pulsar, Voyager) to the right. This way you can follow the high-level flow of each service. Be aware that the Hazelcast caches are the same, they were just split for visual clearness.

![Architecture](./imgs/Horizon-Architecture-Detail.webp)

For a more detailed flowchart, please check the documentation of each component.

As visible in the diagram above, a publisher can start an event flow by sending a POST request to Starlight. This POST request will be the published event. Starlight will check the authentication, authorization, and check the information about the event type against with help of the ENI-Api. After all successful checks, Starlight will create a published event message and write it into the "published" Kafka topic. 

Our Galaxy component listens on that "published" topic and receives that event. Furthermore, it asks the ENI-Api for recipients, schema validation & filters. It applies them and creates a subscription event message for each recipient. This subscription event message gets checked in the De-Duplication Cache, if it already exists, Galaxy will skip the subscription event message, if it was not found in the De-Duplication Cache it will write into the retention-time-based "subscribed" topic with the status PROCESSED and create an entry in the De-Duplication Cache. 

Our delivery component Comet listens on all "subscribed" topics and handles the messages. Once it receives a message, Comet will check if already send that subscription event message in the De-Duplication Cache. Important to know is that the De-Duplication Caches are not shared between Galaxy and Comet, they are only shared between the pods of the components. Each component has its own De-Duplication Cache. If a duplicate was found the event gets flagged as DUPLICATE and skipped. If it was not found Comet will check if a Circuit-Breaker is open for that specific subscription-id. If an open Circuit-Breaker was found, Comet will flag the event as WAITING and skip processing. Else it will flag the event as DELIVERING and send the event to the consumer. If a consumer answers successfully the event gets flagged as DELIVERED. If the consumer answers with a non-retryable error-code the event gets flagged as FAILED. If the consumer answers multiple times with a retry-able error code, the event gets set to WAITING and a Circuit-Breaker for that subscription-id will be opened.

In a configurable interval (ex. 30 sec) Polaris will check for open Circuit-Breakers and handle them. Polaris will split the open Circuit-Breaker messages by hashing the callback-url for the given subscription and applying a modulo with the number of active pods. Therefore only one pod will grab that message and assign itself to it. Afterward, it will spawn a health thread and periodically send a HEAD or GET request (configurable in the subscription) to the consumer's endpoint. Only if the request is successful, Polaris will grab all WAITING events for all corresponding subscriptions, pick them from the correct Kafka topics (subscribed), reset them to PROCESSED and write them again into the subscribed topic. Therefore Comet will pick them up and redeliver them. If a consumer builds a loop, by having an available HEAD endpoint, but an unavailable POST endpoint, Polaris will increase the health check interval up to 1 hour. Meaning in a worst-case, events will only get republished every hour.

This is the event flow for the CALLBACK return type. For the SSE (Server-Sent-Event) return type, Comet will ignore these events and they will stay on PROCESSED. 
If a consumer wants to grab his SSE events, they have to send an SSE request to Pulsar. 
Pulsar will then lookup for the PROCESSED messages of the consumer's subscription. Each message gets checked in the De-Duplication Cache like in the other components. Afterwards, Pulsar will pick the event from the correct Kafka topic (subscribed) and send it over the SSE connection to the consumer. After a successful delivery, the event gets written into the De-Duplication Cache and set to DELIVERED. When all events were sent to the consumer the connection will stay open for 60 more minutes. If no event arrives in that 60 seconds the connection will be closed. Else the incoming events will be forwarded to the consumer. This will repeat until there are no more new events for 60 seconds. 

To view the status of all events, publishers, and consumers can access EventHorizon. 
EventHorizon connects to the EventHorizon-BFF, which loads all data from inside the database. 
Users can also watch their event payload, for that the BFF will pick the events from the correct subscribed topic. 

If a consumer wants to redeliver an event because they may have lost it on their side or it was set to FAILED, the consumer can trigger a republishment by using the Voyager. For that, a consumer can do a GET request and query for specific events. The Voyager will return with the event ids and additional information. With that ids, the consumers can do a POST request to the Voyager. Then Voyager will pick these events in the Kafka and do a republishment like described in Polaris.